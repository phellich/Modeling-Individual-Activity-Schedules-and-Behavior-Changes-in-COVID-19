{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, time\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tabulate import tabulate\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = 'Avenches'\n",
    "# LOCAL = 'Lausanne'\n",
    "group_to_type = {\n",
    "    0: 'home',\n",
    "    1: 'education',\n",
    "    2: 'work',\n",
    "    3: 'leisure',\n",
    "    4: 'shop'\n",
    "}\n",
    "HORIZON = 289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_csv = pd.read_csv(\"Data/PreProcessed/activity.csv\")\n",
    "population_csv = pd.read_csv(\"Data/PreProcessed/population.csv\")\n",
    "NUM_ACTIVITIES = len(activity_csv) + 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_file = 'Data/Original/vaud_activities.csv.gz'\n",
    "population_file = 'Data/Original/vaud_population.csv.gz'\n",
    "trip_file = 'Data/Original/vaud_trips.csv.gz'\n",
    "\n",
    "def read_gzipped_csv(file_path):\n",
    "    with gzip.open(file_path, 'rt') as file:\n",
    "        df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "activity_vaud_full = read_gzipped_csv(activity_file)\n",
    "activity_vaud = activity_vaud_full[activity_vaud_full['type'] != 'pt interaction']\n",
    "population_vaud = read_gzipped_csv(population_file)[['id', 'age', 'home_x', 'home_y', 'local']]\n",
    "# trip_vaud = read_gzipped_csv(trip_file)[['Unnamed: 0', 'id', 'mode', 'dep_time','trav_time','start_link','end_link']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/stable/gallery/color/named_colors.html \n",
    "color_palette = {\n",
    "    'home': 'royalblue',       \n",
    "    'work': 'gold',     \n",
    "    'leisure': 'lightpink', # RGB tuple, each value must be in the range [0, 1]\n",
    "    'shop': 'aquamarine', \n",
    "    'transport': 'silver',   \n",
    "    'pt interaction': 'grey',\n",
    "    'other': 'mediumvioletred',\n",
    "    'education': 'teal',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_schedule_dataset(activity_df, individual_id):\n",
    "    individual_schedule = activity_df[activity_df['id'] == individual_id]\n",
    "    individual_schedule = individual_schedule.sort_index()\n",
    "    \n",
    "    segments = []\n",
    "    colors = []\n",
    "    fig, ax = plt.subplots(figsize=(10, 1))\n",
    "    \n",
    "    start_of_day = datetime.strptime('00:00:00', '%H:%M:%S')\n",
    "    end_of_day = datetime.strptime('23:59:59', '%H:%M:%S')\n",
    "    prev_end = start_of_day\n",
    "    \n",
    "    for _, facility in individual_schedule.iterrows():\n",
    "        start_time = datetime.strptime(facility['start_time'], '%H:%M:%S') if pd.notna(facility['start_time']) else start_of_day\n",
    "        end_time = datetime.strptime(facility['end_time'], '%H:%M:%S') if pd.notna(facility['end_time']) else end_of_day\n",
    "        \n",
    "        if prev_end < start_time:\n",
    "            segments.append([(prev_end - start_of_day).total_seconds() / 3600, (start_time - prev_end).total_seconds() / 3600])\n",
    "            colors.append(color_palette['transport'])\n",
    "            ax.broken_barh([segments[-1]], (0, 1), facecolors=colors[-1])\n",
    "            \n",
    "        segments.append([(start_time - start_of_day).total_seconds() / 3600, (end_time - start_time).total_seconds() / 3600])\n",
    "        colors.append(color_palette[facility['type']])\n",
    "        ax.broken_barh([segments[-1]], (0, 1), facecolors=colors[-1])\n",
    "        \n",
    "        prev_end = end_time\n",
    "\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=activity) for activity, color in color_palette.items()]\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    ax.set_xlim(0, 24)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(range(25))\n",
    "    ax.set_xlabel('Original schedule')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_schedule_json(df, individual_id):\n",
    "    individual_schedule = df[df['id'] == individual_id].sort_values(by='start')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 1))\n",
    "    ax.set_xlim(0, 24)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(range(0, 25, 1))\n",
    "    ax.set_xlabel(f\"Predicted schedule (U={individual_schedule.iloc[0]['utility']:.1f})\")\n",
    "    \n",
    "    # Convert time objects to hours since the start of the day\n",
    "    time_to_hours = lambda t: t.hour + t.minute / 60 + t.second / 3600\n",
    "    \n",
    "    prev_end_time = time_to_hours(datetime.strptime('00:00:00', '%H:%M:%S').time())\n",
    "    \n",
    "    # Iterate over the activities\n",
    "    for _, activity in individual_schedule.iterrows():\n",
    "        start_time = time_to_hours(activity['start'])\n",
    "        end_time = time_to_hours(activity['end_time'])\n",
    "        \n",
    "        # If there is a gap between the previous end time and the current start time, plot it as 'transport'\n",
    "        if prev_end_time < start_time:\n",
    "            ax.broken_barh([(prev_end_time, start_time - prev_end_time)], (0, 1), facecolors=color_palette['transport'])\n",
    "        \n",
    "        ax.broken_barh([(start_time, end_time - start_time)], (0, 1), facecolors=color_palette[activity['group']])\n",
    "        \n",
    "        prev_end_time = end_time\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=activity) for activity, color in color_palette.items()]\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_schedule(activity_df, individual_id):\n",
    "    individual_schedule = activity_df[activity_df['id'] == individual_id]\n",
    "    individual_schedule = individual_schedule.sort_index()\n",
    "    print(tabulate(individual_schedule, headers='keys', tablefmt='pipe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_individual_schedules(df_dataset, df_json, individual_id, table=True, plot=True):\n",
    "    if table: \n",
    "        print(\"Here is his/her schedule in the original dataset :\\n\")\n",
    "        print_schedule(df_dataset, individual_id)\n",
    "    if plot:\n",
    "        plot_schedule_dataset(df_dataset, individual_id)\n",
    "    if table:\n",
    "        print(\"Here is his/her schedule with our planning algorythm :\\n\")\n",
    "        print_schedule(df_json, individual_id)\n",
    "    if plot:\n",
    "        plot_schedule_json(df_json, individual_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizons_to_timedelta(n):\n",
    "    minutes = n * 5\n",
    "    time_delta = timedelta(minutes=minutes)\n",
    "    return time_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_flat_dataframe(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = pd.read_json(file)\n",
    "    \n",
    "    # Normalize the 'daily_schedule' data to create a flat table\n",
    "    # We will concatenate all the schedules along with their top-level data such as 'id', 'execution_time', etc.\n",
    "    flat_data = pd.DataFrame()  # Empty dataframe to hold our flattened data\n",
    "    for record in data.to_dict(orient='records'):\n",
    "        # Normalize the daily_schedule for each record\n",
    "        schedule_df = json_normalize(record, 'daily_schedule', errors='ignore')\n",
    "        \n",
    "        # Adding the top-level data as new columns to the schedule_df\n",
    "        for col in data.columns.difference(['daily_schedule']):\n",
    "            schedule_df[col] = record[col]\n",
    "        \n",
    "        # Append to the flat_data DataFrame\n",
    "        flat_data = pd.concat([flat_data, schedule_df], ignore_index=True)\n",
    "    \n",
    "    return flat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minutes_to_time2(minute_value):\n",
    "    total_minutes = minute_value * 5\n",
    "    hours, minutes = divmod(total_minutes, 60)\n",
    "    if hours >= 24:\n",
    "        hours = hours % 24\n",
    "    return time(hour=int(hours), minute=int(minutes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_sort_activities(df):\n",
    "    # Convert 'start' and 'duration' from time objects to timedeltas for comparison\n",
    "    df['duration_td'] = df['duration'].apply(lambda x: pd.to_timedelta(x.strftime('%H:%M:%S')))\n",
    "    \n",
    "    # Group by 'acity' and find the index of the row with the maximum duration\n",
    "    idx = df.groupby(['acity', 'id', 'start'])['duration_td'].idxmax()\n",
    "    \n",
    "    # Select only the rows with the maximum duration\n",
    "    max_duration_df = df.loc[idx]\n",
    "    \n",
    "    # Drop the auxiliary 'duration_td' column\n",
    "    max_duration_df = max_duration_df.drop(columns=['duration_td'])\n",
    "    \n",
    "    # Now, sort the resulting DataFrame by the 'start' column\n",
    "    max_duration_df = max_duration_df.sort_values(by='start', ascending = True)\n",
    "    \n",
    "    return max_duration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert a time object to minutes\n",
    "def time_to_minutes(t):\n",
    "    return t.hour * 60 + t.minute\n",
    "\n",
    "# Helper function to convert minutes to a time object\n",
    "def minutes_to_time(minutes):\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return time(hour=hours, minute=minutes)\n",
    "\n",
    "# Function to calculate the end time of activities\n",
    "def calculate_end_time(row):\n",
    "\n",
    "    start_minutes = time_to_minutes(row['start'])\n",
    "    duration_minutes = time_to_minutes(row['duration'])\n",
    "    end_minutes = start_minutes + duration_minutes\n",
    "\n",
    "    # Handle the special case for the last row\n",
    "    if end_minutes >= 1440:\n",
    "        return time(23, 59, 0)\n",
    "\n",
    "    return minutes_to_time(end_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_df(df):\n",
    "    new_order = [\n",
    "        'acity', 'facility', 'group', 'start', 'duration', 'end_time', 'time',\n",
    "        'cum_utility', 'DSSR_iterations', 'execution_time', 'id', 'utility'\n",
    "    ]\n",
    "    df = df[new_order]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_schedules_json(df): \n",
    "    ''' To apply on json file output by the algo \n",
    "    Return clean dataframe all schedules '''\n",
    "    \n",
    "    df['start'] = df['start'].apply(minutes_to_time2)\n",
    "    df['duration'] = df['duration'].apply(minutes_to_time2)\n",
    "    df['time'] = df['time'].apply(minutes_to_time2)\n",
    "\n",
    "    filtered_df = filter_and_sort_activities(df)\n",
    "\n",
    "    filtered_df['end_time'] = filtered_df.apply(calculate_end_time, axis=1)\n",
    "\n",
    "    schedule_postprocessed = reorder_df(filtered_df)\n",
    "\n",
    "    return schedule_postprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hours_list(row):\n",
    "    ''' Function to create list of hours covered by the activity '''\n",
    "    start_hour = row['start_datetime'].hour\n",
    "    end_hour = row['end_datetime'].hour + (1 if row['end_datetime'].minute > 0 else 0)  # adjust if minutes > 0\n",
    "    return list(range(start_hour, end_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_observed_ToD_activity_frequency(df, col_name):\n",
    "    \"\"\" Plot frequentation of each activity through the day for the MATSim schedules\"\"\"\n",
    "    if col_name == 'group':\n",
    "        df = df[df[col_name] != 'home']\n",
    "\n",
    "    # Apply the function and create a new 'hours' column\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    df.loc[:, 'hours'] = df.apply(get_hours_list, axis=1)\n",
    "\n",
    "    # Explode 'hours' so each hour is a separate row\n",
    "    df_hours_exploded = df.explode('hours')\n",
    "\n",
    "    # Count occurrences\n",
    "    hour_activity_count = df_hours_exploded.groupby(['hours', col_name]).size().unstack(fill_value=0)\n",
    "    all_hours = pd.Index(range(24), name='hours')\n",
    "    hour_activity_count = hour_activity_count.reindex(all_hours, fill_value=0)\n",
    "\n",
    "    # Calculate proportions\n",
    "    total_people = df['id'].nunique()\n",
    "    hour_activity_proportion = hour_activity_count.div(total_people)\n",
    "\n",
    "    return hour_activity_proportion\n",
    "\n",
    "def plot_observed_ToD_activity_frequency(df):\n",
    "    \"\"\" Plot frequentation of each activity through the day for the generated schedules\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Construire une liste de couleurs basée sur les colonnes actuelles du DataFrame\n",
    "    column_colors = [color_palette.get(activity, 'gray') for activity in df.columns]\n",
    "\n",
    "    # Create a stacked bar chart\n",
    "    df.plot(kind='bar', stacked=True, ax=ax, color=column_colors)\n",
    "\n",
    "    ax.set_title('Distribution (data)')\n",
    "    ax.set_xlabel('Time [h]')\n",
    "    ax.set_ylabel('Proportion')\n",
    "\n",
    "    plt.xticks(ticks=range(24), labels=[f'{hour}:00' for hour in range(24)], rotation=0)\n",
    "    ax.legend(title='Activity Type', loc='upper right', bbox_to_anchor=(1.0, 1.0))\n",
    "    plt.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_timedelta(t):\n",
    "    if pd.isnull(t):\n",
    "        return pd.Timedelta(seconds=0)\n",
    "    return pd.Timedelta(hours=t.hour, minutes=t.minute, seconds=t.second)\n",
    "\n",
    "def calculate_mean_duration_hm(df, col_name, p=False):\n",
    "    ''' calculate duration of each activty in an average schedule'''\n",
    "    \n",
    "    # Calculate the mean duration by type as total seconds\n",
    "    if col_name == 'group':\n",
    "        df['duration'] = df['duration'].apply(time_to_timedelta)\n",
    "    mean_duration_by_type = df.groupby(col_name)['duration'].mean()\n",
    "    \n",
    "    # Convert the mean duration to the format 'HH:MM'\n",
    "    mean_duration_by_type_hm = mean_duration_by_type.apply(\n",
    "        lambda x: f\"{int(x.total_seconds() // 3600):02d}:{int((x.total_seconds() % 3600) // 60):02d}\"\n",
    "    )\n",
    "\n",
    "    if p:\n",
    "        print(mean_duration_by_type_hm)\n",
    "    \n",
    "    return mean_duration_by_type_hm\n",
    "\n",
    "def calculate_activity_proportions(df, col_type_name, p=False):\n",
    "    ''' Calculate the proportion of schedules containing each activty'''\n",
    "    # Filter the DataFrame for relevant activity types if needed\n",
    "    activity_types = ['shop', 'leisure', 'work', 'education']\n",
    "    filtered_df = df[df[col_type_name].isin(activity_types)]\n",
    "    \n",
    "    # Group by 'id' and 'type' and check if the activity exists in the schedule\n",
    "    activity_by_individual = filtered_df.groupby(['id', col_type_name]).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Now check for the presence of each activity, result is boolean\n",
    "    activity_presence = activity_by_individual > 0\n",
    "\n",
    "    # Calculate the proportion of individuals that have each activity\n",
    "    activity_proportions = activity_presence.mean()\n",
    "    \n",
    "    # Convert the proportions to a DataFrame for nice formatting\n",
    "    proportions_df = activity_proportions\n",
    "    proportions_df.columns = ['Activity Type', 'Proportion']\n",
    "    \n",
    "    if p: \n",
    "        print(proportions_df)\n",
    "        \n",
    "    return proportions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json_file = \"Data/Generated/schedules.json\"\n",
    "results = json_to_flat_dataframe(path_to_json_file)\n",
    "results_clean = process_schedules_json(results)\n",
    "# print(set(df['id']))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, id in enumerate(set(results_clean['id'])):\n",
    "    if index == 5:\n",
    "        break\n",
    "    print(f\"\\n SCHEDULE OF INDIVIDUAL {id} (n*{index}) \\n\")\n",
    "    compare_individual_schedules(activity_vaud, results_clean, id, table=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = 6732548 \n",
    "# compare_individual_schedules(activity_vaud, results_clean, id, table=True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "sample_pop = pd.read_csv(\"Data/PreProcessed/population.csv\")\n",
    "sample_ids = sample_pop['id']\n",
    "orig_sched_samp = activity_vaud[activity_vaud['id'].isin(sample_ids)] \n",
    "orig_sched_samp_filt = orig_sched_samp[~orig_sched_samp['type'].isin(['pt interaction', 'home'])] # pas filtrer others pour la coherence\n",
    "# peut etre filtrer les temps superieurs a 24h ? \n",
    "# cf preprocessing\n",
    "orig_sched_samp_filt.loc[:, 'start_datetime'] = pd.to_datetime(\n",
    "    orig_sched_samp_filt['start_time'], \n",
    "    errors='coerce', \n",
    "    format='%H:%M:%S' \n",
    ").fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "\n",
    "orig_sched_samp_filt.loc[:, 'end_datetime'] = pd.to_datetime(\n",
    "    orig_sched_samp_filt['end_time'], \n",
    "    errors='coerce', \n",
    "    format='%H:%M:%S'  \n",
    ").fillna(pd.Timestamp('1900-01-01 23:59:00'))\n",
    "\n",
    "orig_sched_samp_filt['duration'] = orig_sched_samp_filt['end_datetime'] - orig_sched_samp_filt['start_datetime']\n",
    "\n",
    "hour_activity_proportion = construct_observed_ToD_activity_frequency(orig_sched_samp_filt, 'type')\n",
    "plot_observed_ToD_activity_frequency(hour_activity_proportion)\n",
    "# orig_sched_samp_filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clean.loc[:, 'start_datetime'] = pd.to_datetime(\n",
    "    results_clean['start'], \n",
    "    errors='coerce', \n",
    "    format='%H:%M:%S' \n",
    ").fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "results_clean.loc[:, 'end_datetime'] = pd.to_datetime(\n",
    "    results_clean['end_time'], \n",
    "    errors='coerce', \n",
    "    format='%H:%M:%S'  \n",
    ").fillna(pd.Timestamp('1900-01-01 23:59:00'))\n",
    "\n",
    "hour_activity_proportion_2 =construct_observed_ToD_activity_frequency(results_clean, 'group')\n",
    "plot_observed_ToD_activity_frequency(hour_activity_proportion_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mean_duration_hm(orig_sched_samp_filt, 'type', p=True)\n",
    "calculate_mean_duration_hm(results_clean, 'group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_activity_proportions(orig_sched_samp_filt, 'type', p=True)\n",
    "calculate_activity_proportions(results_clean, 'group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schedule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
