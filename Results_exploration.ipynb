{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, time\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tabulate import tabulate\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = 'Avenches'\n",
    "# LOCAL = 'Lausanne'\n",
    "group_to_type = {\n",
    "    0: 'home',\n",
    "    1: 'education',\n",
    "    2: 'work',\n",
    "    3: 'leisure',\n",
    "    4: 'shop'\n",
    "}\n",
    "TIME_INTERVAL = 5\n",
    "HORIZON = round(24*60/TIME_INTERVAL) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_csv = pd.read_csv(f\"Data/2_PreProcessed/activities_{LOCAL}.csv\")\n",
    "population_csv = pd.read_csv(f\"Data/2_PreProcessed/population_{LOCAL}.csv\")\n",
    "NUM_ACTIVITIES = len(activity_csv) + 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_path = 'Data/1_Original/vaud_activities.csv.gz'\n",
    "population_path = 'Data/1_Original/vaud_population.csv.gz'\n",
    "\n",
    "def read_gzipped_csv(file_path):\n",
    "    with gzip.open(file_path, 'rt') as file:\n",
    "        df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "activity_vaud_full = read_gzipped_csv(activities_path)\n",
    "activity_vaud = activity_vaud_full[activity_vaud_full['type'] != 'pt interaction']\n",
    "population_vaud = read_gzipped_csv(population_path)[['id', 'age', 'home_x', 'home_y', 'local']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/stable/gallery/color/named_colors.html \n",
    "color_palette = {\n",
    "    'home': 'royalblue',       \n",
    "    'work': 'gold',     \n",
    "    'leisure': 'lightpink', \n",
    "    'shop': 'aquamarine', \n",
    "    'transport': 'silver',   \n",
    "    'pt interaction': 'grey',\n",
    "    'other': 'mediumvioletred',\n",
    "    'education': 'teal',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_schedule_dataset(activity_df, individual_id):\n",
    "    individual_schedule = activity_df[activity_df['id'] == individual_id]\n",
    "    individual_schedule = individual_schedule.sort_index()\n",
    "    \n",
    "    segments = []\n",
    "    colors = []\n",
    "    fig, ax = plt.subplots(figsize=(10, 1))\n",
    "    \n",
    "    start_of_day = datetime.strptime('00:00:00', '%H:%M:%S')\n",
    "    end_of_day = datetime.strptime('23:59:59', '%H:%M:%S')\n",
    "    prev_end = start_of_day\n",
    "    \n",
    "    for _, facility in individual_schedule.iterrows():\n",
    "        start_time = datetime.strptime(facility['start_time'], '%H:%M:%S') if pd.notna(facility['start_time']) else start_of_day\n",
    "        end_time = datetime.strptime(facility['end_time'], '%H:%M:%S') if pd.notna(facility['end_time']) else end_of_day\n",
    "        \n",
    "        if prev_end < start_time:\n",
    "            segments.append([(prev_end - start_of_day).total_seconds() / 3600, (start_time - prev_end).total_seconds() / 3600])\n",
    "            colors.append(color_palette['transport'])\n",
    "            ax.broken_barh([segments[-1]], (0, 1), facecolors=colors[-1])\n",
    "            \n",
    "        segments.append([(start_time - start_of_day).total_seconds() / 3600, (end_time - start_time).total_seconds() / 3600])\n",
    "        colors.append(color_palette[facility['type']])\n",
    "        ax.broken_barh([segments[-1]], (0, 1), facecolors=colors[-1])\n",
    "        \n",
    "        prev_end = end_time\n",
    "\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=activity) for activity, color in color_palette.items()]\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    ax.set_xlim(0, 24)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(range(25))\n",
    "    ax.set_xlabel('Original schedule')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_schedule_json(df, individual_id, title=None):\n",
    "    individual_schedule = df[df['id'] == individual_id].sort_values(by='start')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 1))\n",
    "    ax.set_xlim(0, 24)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(range(0, 25, 1))\n",
    "    ax.set_xlabel(f\"Scenario {title} (U={individual_schedule.iloc[0]['utility']:.1f})\")\n",
    "    \n",
    "    # Convert time objects to hours since the start of the day\n",
    "    time_to_hours = lambda t: t.hour + t.minute / 60 + t.second / 3600\n",
    "    \n",
    "    prev_end_time = time_to_hours(datetime.strptime('00:00:00', '%H:%M:%S').time())\n",
    "    \n",
    "    # Iterate over the activities\n",
    "    for _, activity in individual_schedule.iterrows():\n",
    "        start_time = time_to_hours(activity['start'])\n",
    "        end_time = time_to_hours(activity['end_time'])\n",
    "        \n",
    "        # If there is a gap between the previous end time and the current start time, plot it as 'transport'\n",
    "        if prev_end_time < start_time:\n",
    "            ax.broken_barh([(prev_end_time, start_time - prev_end_time)], (0, 1), facecolors=color_palette['transport'])\n",
    "        \n",
    "        ax.broken_barh([(start_time, end_time - start_time)], (0, 1), facecolors=color_palette[activity['group']])\n",
    "        \n",
    "        prev_end_time = end_time\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=activity) for activity, color in color_palette.items()]\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_schedule(activity_df, individual_id):\n",
    "    individual_schedule = activity_df[activity_df['id'] == individual_id]\n",
    "    individual_schedule = individual_schedule.sort_index()\n",
    "    print(tabulate(individual_schedule, headers='keys', tablefmt='pipe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_individual_schedules(df_dataset, df_json, individual_id, table=True, plot=True):\n",
    "    if table: \n",
    "        print(\"Here is his/her schedule in the original dataset :\\n\")\n",
    "        print_schedule(df_dataset, individual_id)\n",
    "    if plot:\n",
    "        plot_schedule_dataset(df_dataset, individual_id)\n",
    "    if table:\n",
    "        print(\"Here is his/her schedule with our planning algorythm :\\n\")\n",
    "        print_schedule(df_json, individual_id)\n",
    "    if plot:\n",
    "        plot_schedule_json(df_json, individual_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_flat_dataframe(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = pd.read_json(file)\n",
    "    \n",
    "    # Normalize the 'daily_schedule' data to create a flat table\n",
    "    # We will concatenate all the schedules along with their top-level data such as 'id', 'execution_time', etc.\n",
    "    flat_data = pd.DataFrame()  # Empty dataframe to hold our flattened data\n",
    "    for record in data.to_dict(orient='records'):\n",
    "        # Normalize the daily_schedule for each record\n",
    "        schedule_df = json_normalize(record, 'daily_schedule', errors='ignore')\n",
    "        \n",
    "        # Adding the top-level data as new columns to the schedule_df\n",
    "        for col in data.columns.difference(['daily_schedule']):\n",
    "            schedule_df[col] = record[col]\n",
    "        \n",
    "        # Append to the flat_data DataFrame\n",
    "        flat_data = pd.concat([flat_data, schedule_df], ignore_index=True)\n",
    "    \n",
    "    return flat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_sort_activities(df):\n",
    "    # Convert 'start' and 'duration' from time objects to timedeltas for comparison\n",
    "    df['duration_td'] = df['duration'].apply(lambda x: pd.to_timedelta(x.strftime('%H:%M:%S')))\n",
    "    \n",
    "    # Group by 'acity' and find the index of the row with the maximum duration\n",
    "    idx = df.groupby(['acity', 'id', 'start'])['duration_td'].idxmax()\n",
    "    \n",
    "    # Select only the rows with the maximum duration\n",
    "    max_duration_df = df.loc[idx]\n",
    "    \n",
    "    # Drop the auxiliary 'duration_td' column\n",
    "    max_duration_df = max_duration_df.drop(columns=['duration_td'])\n",
    "    \n",
    "    # Now, sort the resulting DataFrame by the 'start' column\n",
    "    max_duration_df = max_duration_df.sort_values(by='start', ascending = True)\n",
    "    \n",
    "    return max_duration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert a time object to minutes\n",
    "def time_to_minutes(t):\n",
    "    return t.hour * 60 + t.minute\n",
    "\n",
    "# Helper function to convert minutes to a time object\n",
    "def minutes_to_time(minutes):\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return time(hour=hours, minute=minutes)\n",
    "\n",
    "# Function to calculate the end time of activities\n",
    "def calculate_end_time(row):\n",
    "\n",
    "    start_minutes = time_to_minutes(row['start'])\n",
    "    duration_minutes = time_to_minutes(row['duration'])\n",
    "    end_minutes = start_minutes + duration_minutes\n",
    "\n",
    "    # Handle the special case for the last row\n",
    "    if end_minutes >= 1440:\n",
    "        return time(23, 59, 0)\n",
    "\n",
    "    return minutes_to_time(end_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_df(df):\n",
    "    new_order = [\n",
    "        'acity', 'facility', 'group', 'start', 'duration', 'end_time', 'time', \n",
    "        'total_deviation_start', 'total_deviation_dur', 'deviation_start', 'deviation_dur',\n",
    "        'cum_utility', 'DSSR_iterations', 'execution_time', 'id', 'utility'\n",
    "    ]\n",
    "    df = df[new_order]\n",
    "    return df\n",
    "\n",
    "def minutes_to_time2(minute_value):\n",
    "    total_minutes = minute_value * TIME_INTERVAL\n",
    "    hours, minutes = divmod(total_minutes, 60)\n",
    "    if hours >= 24:\n",
    "        hours = hours % 24\n",
    "    return time(hour=int(hours), minute=int(minutes))\n",
    "\n",
    "def process_schedules_json(df): \n",
    "    ''' To apply on json file output by the algo \n",
    "    Return clean dataframe all schedules '''\n",
    "    \n",
    "    df['start'] = df['start'].apply(minutes_to_time2)\n",
    "    df['duration'] = df['duration'].apply(minutes_to_time2)\n",
    "    df['time'] = df['time'].apply(minutes_to_time2)\n",
    "\n",
    "    filtered_df = filter_and_sort_activities(df)\n",
    "\n",
    "    filtered_df['end_time'] = filtered_df.apply(calculate_end_time, axis=1)\n",
    "\n",
    "    schedule_postprocessed = reorder_df(filtered_df)\n",
    "\n",
    "    return schedule_postprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hours_list(row):\n",
    "    ''' Function to create a list of hours covered by the activity '''\n",
    "    start_hour = row['start_datetime'].hour\n",
    "    end_hour = row['end_datetime'].hour + (1 if row['end_datetime'].minute > 0 else 0)  # adjust if minutes > 0\n",
    "    hours_list = list(range(start_hour, end_hour))\n",
    "    return hours_list\n",
    "\n",
    "def construct_observed_ToD_activity_frequency(df, col_name):\n",
    "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Filter out 'home' if col_name is 'group'\n",
    "    if col_name == 'group':\n",
    "        df_copy = df_copy[df_copy[col_name] != 'home']\n",
    "\n",
    "    if df_copy.empty:  # Handle the empty DataFrame case\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Apply the function to create a new 'hours' column\n",
    "    df_copy['hours'] = df_copy.apply(lambda row: get_hours_list(row), axis=1)\n",
    "\n",
    "    # Explode the 'hours' column so each hour is a separate row\n",
    "    df_exploded = df_copy.explode('hours')\n",
    "\n",
    "    # Count occurrences\n",
    "    hour_activity_count = df_exploded.groupby(['hours', col_name]).size().unstack(fill_value=0)\n",
    "    all_hours = pd.Index(range(24), name='hours')\n",
    "    hour_activity_count = hour_activity_count.reindex(all_hours, fill_value=0)\n",
    "\n",
    "    # Calculate proportions\n",
    "    total_people = df_copy['id'].nunique()\n",
    "    hour_activity_proportion = hour_activity_count.div(total_people)\n",
    "\n",
    "    return hour_activity_proportion\n",
    "\n",
    "def plot_observed_ToD_activity_frequency(df, scenario= None):\n",
    "    \"\"\" Plot frequentation of each activity through the day for the generated schedules\"\"\"\n",
    "\n",
    "    if df.empty: # Handle the empty DataFrame case, maybe return an empty DataFrame or similar\n",
    "        print(\"EVERYBODY STAY AT HOME !\\n\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 5))\n",
    "    \n",
    "    # Construire une liste de couleurs basée sur les colonnes actuelles du DataFrame\n",
    "    column_colors = [color_palette.get(activity, 'gray') for activity in df.columns]\n",
    "\n",
    "    # Create a stacked bar chart\n",
    "    df.plot(kind='bar', stacked=True, ax=ax, color=column_colors)\n",
    "\n",
    "    ax.set_title(scenario)\n",
    "    ax.set_xlabel('Time [h]')\n",
    "    ax.set_ylabel('Proportion')\n",
    "\n",
    "    plt.xticks(ticks=range(24), labels=[f'{hour}:00' for hour in range(24)], rotation=0)\n",
    "    ax.legend(title='Activity Type', loc='upper right', bbox_to_anchor=(1.0, 1.0))\n",
    "    plt.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_timedelta(t):\n",
    "    if pd.isnull(t):\n",
    "        return pd.Timedelta(seconds=0)\n",
    "    return pd.Timedelta(hours=t.hour, minutes=t.minute, seconds=t.second)\n",
    "\n",
    "def calculate_mean_duration_hm(df, col_name, p=False):\n",
    "    ''' Calculate duration of each activty in an average schedule'''\n",
    "    \n",
    "    # Calculate the mean duration by type as total seconds\n",
    "    if col_name == 'group':\n",
    "        df['duration'] = df['duration'].apply(time_to_timedelta)\n",
    "    \n",
    "    df = df[df[col_name] != 'home']\n",
    "    mean_duration_by_type = df.groupby(col_name)['duration'].mean()\n",
    "    \n",
    "    # Convert the mean duration to the format 'HH:MM'\n",
    "    mean_duration_by_type_hm = mean_duration_by_type.apply(\n",
    "        lambda x: f\"{int(x.total_seconds() // 3600):02d}:{int((x.total_seconds() % 3600) // 60):02d}\"\n",
    "    )\n",
    "\n",
    "    if p:\n",
    "        print(mean_duration_by_type_hm)\n",
    "    \n",
    "    return mean_duration_by_type_hm\n",
    "\n",
    "def calculate_mean_start_hm(df, col_name, p=False):\n",
    "    ''' Calculate start of each activty in an average schedule'''\n",
    "    \n",
    "    # Calculate the mean duration by type as total seconds\n",
    "    if col_name == 'group':\n",
    "        df['start'] = df['start'].apply(time_to_timedelta)\n",
    "    \n",
    "    df = df[df[col_name] != 'home']\n",
    "    mean_duration_by_type = df.groupby(col_name)['start'].mean()\n",
    "    \n",
    "    # Convert the mean duration to the format 'HH:MM'\n",
    "    mean_duration_by_type_hm = mean_duration_by_type.apply(\n",
    "        lambda x: f\"{int(x.total_seconds() // 3600):02d}:{int((x.total_seconds() % 3600) // 60):02d}\"\n",
    "    )\n",
    "\n",
    "    if p:\n",
    "        print(mean_duration_by_type_hm)\n",
    "    \n",
    "    return mean_duration_by_type_hm\n",
    "\n",
    "def calculate_activity_proportions(df, col_type_name, p=False):\n",
    "    ''' Calculate the proportion of schedules containing each activty'''\n",
    "    # Filter the DataFrame for relevant activity types if needed\n",
    "    activity_types = ['shop', 'leisure', 'work', 'education']\n",
    "    filtered_df = df[df[col_type_name].isin(activity_types)]\n",
    "    \n",
    "    # Group by 'id' and 'type' and check if the activity exists in the schedule\n",
    "    activity_by_individual = filtered_df.groupby(['id', col_type_name]).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Now check for the presence of each activity, result is boolean\n",
    "    activity_presence = activity_by_individual > 0\n",
    "\n",
    "    # Calculate the proportion of individuals that have each activity\n",
    "    activity_proportions = round(activity_presence.mean(), 2)\n",
    "    \n",
    "    # Convert the proportions to a DataFrame for nice formatting\n",
    "    proportions_df = activity_proportions\n",
    "    proportions_df.columns = ['Activity Type', 'Proportion']\n",
    "    \n",
    "    if p: \n",
    "        print(proportions_df)\n",
    "        \n",
    "    return proportions_df\n",
    "\n",
    "def calculate_deviation(df):\n",
    "\n",
    "    df_copy = df.drop_duplicates(subset=[\"id\"], keep='first', inplace = False)\n",
    "\n",
    "    mean_dev_start = df_copy['total_deviation_start'].mean()\n",
    "    std_dev_start = df_copy['total_deviation_start'].std()\n",
    "    median_dev_start = df_copy['total_deviation_start'].median()\n",
    "    mean_dev_dur = df_copy['total_deviation_dur'].mean()\n",
    "    std_dev_dur = df_copy['total_deviation_dur'].std()\n",
    "    median_dev_dur = df_copy['total_deviation_dur'].median()\n",
    "\n",
    "    mean_dev_start_hm = minutes_to_time2(mean_dev_start)\n",
    "    std_dev_start_hm = minutes_to_time2(std_dev_start)\n",
    "    median_dev_start_hm = minutes_to_time2(median_dev_start)\n",
    "    mean_dev_dur_hm = minutes_to_time2(mean_dev_dur)\n",
    "    std_dev_dur_hm = minutes_to_time2(std_dev_dur)\n",
    "    median_dev_dur_hm = minutes_to_time2(median_dev_dur)\n",
    "    \n",
    "    print(\"Statistics for 'total_deviation_start':\")\n",
    "    print(\"Mean: \", mean_dev_start_hm)\n",
    "    print(\"Std dev: \", std_dev_start_hm)\n",
    "    print(\"Median: \", median_dev_start_hm)\n",
    "    print(\"\\nStatistics for 'total_deviation_dur':\")\n",
    "    print(\"Mean: \", mean_dev_dur_hm)\n",
    "    print(\"Std dev: \", std_dev_dur_hm)\n",
    "    print(\"Median: \", median_dev_dur_hm)\n",
    "\n",
    "    return mean_dev_start_hm, std_dev_start_hm, median_dev_start_hm, mean_dev_dur_hm, std_dev_dur_hm, median_dev_dur_hm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal life scenario analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenario_json(scenario):\n",
    "    path_to_json_file = f\"Data/3_Generated/{scenario}.json\"\n",
    "    results = json_to_flat_dataframe(path_to_json_file)\n",
    "    results_clean = process_schedules_json(results)\n",
    "    return results_clean\n",
    "\n",
    "def compare_scenario(\n",
    "        scenarios, print_table = False, \n",
    "        plot_schedule=False, \n",
    "        n_ind = 0, \n",
    "        plot_distribution = False, \n",
    "        print_proportions= False, \n",
    "        print_durations=False, \n",
    "        print_start_times=False, \n",
    "        print_deviation=False\n",
    "    ):\n",
    "    \n",
    "    all_df = {}\n",
    "    for scenario in scenarios:\n",
    "\n",
    "        print(f\"\\nRESULTS OF {scenario} : \")\n",
    "        all_df[scenario] = process_scenario_json(scenario)\n",
    "        df = all_df[scenario]\n",
    "\n",
    "        for index, id in enumerate(set(df['id'])):\n",
    "            if index == n_ind:\n",
    "                break\n",
    "            if print_table:\n",
    "                print(f\"\\n SCHEDULE OF INDIVIDUAL {id} (n*{index}) \\n\")\n",
    "                print_schedule(df, id)\n",
    "            if plot_schedule:\n",
    "                plot_schedule_json(df, id, scenario)\n",
    "            \n",
    "        df.loc[:, 'start_datetime'] = pd.to_datetime(\n",
    "            df['start'], \n",
    "            errors='coerce', \n",
    "            format='%H:%M:%S' \n",
    "        ).fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "        df.loc[:, 'end_datetime'] = pd.to_datetime(\n",
    "            df['end_time'], \n",
    "            errors='coerce', \n",
    "            format='%H:%M:%S'  \n",
    "        ).fillna(pd.Timestamp('1900-01-01 23:59:00'))\n",
    "\n",
    "        if plot_distribution:\n",
    "            hour_activity_proportion = construct_observed_ToD_activity_frequency(df, 'group')\n",
    "            plot_observed_ToD_activity_frequency(hour_activity_proportion, scenario)\n",
    "\n",
    "        if print_proportions:\n",
    "            calculate_activity_proportions(df, 'group', p=True)\n",
    "\n",
    "        if print_durations:\n",
    "            calculate_mean_duration_hm(df, 'group', p=True)\n",
    "\n",
    "        if print_start_times: # does this even make sense ? \n",
    "            calculate_mean_start_hm(df, 'group', p=True)\n",
    "\n",
    "        if print_deviation: \n",
    "            calculate_deviation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARI = ['Normal_life', 'Outings_limitation', 'Only_economy', 'Early_curfew', \n",
    "            'Essential_needs', 'Finding_balance', 'Impact_of_leisure']\n",
    "# SCENARI = ['Normal_life']\n",
    "# compare_scenario(SCENARI, plot_distribution = True)\n",
    "# compare_scenario(SCENARI, print_proportions= True, print_durations=True)\n",
    "compare_scenario(SCENARI, print_deviation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCENARI = ['Normal_life_TW', 'Outings_limitation_TW', 'Only_economy_TW', 'Early_curfew_TW', \n",
    "#             'Essential_needs_TW', 'Finding_balance_TW', 'Impact_of_leisure_TW']\n",
    "# compare_scenario(SCENARI, plot_distribution = True)\n",
    "# compare_scenario(SCENARI, print_proportions= True, print_durations=True)\n",
    "# compare_scenario(SCENARI, print_deviation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCENARI = ['Normal_life_TW2', 'Outings_limitation_TW2', 'Only_economy_TW2', 'Early_curfew_TW2', \n",
    "#             'Essential_needs_TW2', 'Finding_balance_TW2', 'Impact_of_leisure_TW2']\n",
    "# compare_scenario(SCENARI, plot_distribution = True)\n",
    "# compare_scenario(SCENARI, print_proportions= True, print_durations=True)\n",
    "# compare_scenario(SCENARI, print_deviation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCENARI = ['Normal_life_HEUR', 'Outings_limitation_HEUR', 'Only_economy_HEUR', 'Early_curfew_HEUR', \n",
    "#             'Essential_needs_HEUR', 'Finding_balance_HEUR', 'Impact_of_leisure_HEUR']\n",
    "# compare_scenario(SCENARI, plot_distribution = True)\n",
    "# compare_scenario(SCENARI, print_proportions= True, print_durations=True)\n",
    "# compare_scenario(SCENARI, print_deviation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARI = ['Normal_life', 'Outings_limitation', 'Only_economy', 'Early_curfew', \n",
    "            'Essential_needs', 'Finding_balance', 'Impact_of_leisure']\n",
    "\n",
    "def compare_individual_scenari(id, scenari):\n",
    "    '''\n",
    "    Input : the id of an individual, and all scenari you want to see for this id\n",
    "    Return: plot the schedule of this id for all the scenario in scenari\n",
    "    '''\n",
    "    print(f\"People id : {id}\")\n",
    "    for i, scenario in enumerate(scenari):\n",
    "        # if index == 1:\n",
    "        #     break\n",
    "        df = json_to_flat_dataframe(f\"Data/3_Generated/{scenario}.json\")\n",
    "        df_clean = process_schedules_json(df)\n",
    "        plot_schedule_json(df_clean, id, scenario)\n",
    "\n",
    "def explore_schedules_adaptation(scenari = SCENARI, base_scenario = 'Normal_life', i_break = 0, n_act_in_normal_day=3):\n",
    "    '''\n",
    "    Use this function to explore the results to find interessant schedules adaptation for some id\n",
    "    '''\n",
    "    normal_life_df = process_scenario_json(base_scenario)\n",
    "    for index, id in enumerate(set(normal_life_df['id'])):\n",
    "        if index >= i_break: \n",
    "            break\n",
    "        perso_df = normal_life_df[normal_life_df['id']==id]\n",
    "        nb_of_facilities = perso_df['group'].nunique()\n",
    "        if nb_of_facilities == n_act_in_normal_day: \n",
    "            # plot_schedule_json(perso_df, id, 'Normal_life')\n",
    "            # print_schedule(perso_df, id)\n",
    "            compare_individual_scenari(id, scenari)\n",
    "\n",
    "# explore_schedules_adaptation(scenari = SCENARI, base_scenario = 'Normal_life', i_break = 0, n_act_in_normal_day=3)\n",
    "\n",
    "# Check again the id I found cool\n",
    "relevant_ids = [6682641, 6326330, 6326356]\n",
    "for id in relevant_ids:\n",
    "    compare_individual_scenari(id, SCENARI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATSIM DATA OF GLOBAL POPULATION\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "sample_pop = pd.read_csv(f\"Data/2_PreProcessed/population_{LOCAL}.csv\")\n",
    "sample_ids = sample_pop['id']\n",
    "orig_sched_samp = activity_vaud[activity_vaud['id'].isin(sample_ids)] \n",
    "orig_sched_samp_filt = orig_sched_samp[~orig_sched_samp['type'].isin(['pt interaction', 'home'])] \n",
    "orig_sched_samp_filt.loc[:, 'start_datetime'] = pd.to_datetime(\n",
    "    orig_sched_samp_filt['start_time'], \n",
    "    errors='coerce', \n",
    "    format='%H:%M:%S' \n",
    ").fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "\n",
    "orig_sched_samp_filt.loc[:, 'end_datetime'] = pd.to_datetime(\n",
    "    orig_sched_samp_filt['end_time'], \n",
    "    errors='coerce', \n",
    "    format='%H:%M:%S'  \n",
    ").fillna(pd.Timestamp('1900-01-01 23:59:00'))\n",
    "\n",
    "orig_sched_samp_filt['duration'] = orig_sched_samp_filt['end_datetime'] - orig_sched_samp_filt['start_datetime']\n",
    "\n",
    "hour_activity_proportion = construct_observed_ToD_activity_frequency(orig_sched_samp_filt, 'type')\n",
    "plot_observed_ToD_activity_frequency(hour_activity_proportion)\n",
    "# orig_sched_samp_filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATSIM DATA OF STUDENT POPULATION\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "population_local = population_vaud[population_vaud['local'] == LOCAL] \n",
    "students_ids = population_local[(population_local['age'] >= 10) & (population_local['age'] <= 18)]['id']\n",
    "students_schedule = activity_vaud[activity_vaud['id'].isin(students_ids)] \n",
    "students_schedule_filt = students_schedule[~students_schedule['type'].isin(['pt interaction', 'home'])] \n",
    "\n",
    "students_schedule_filt.loc[:, 'start_datetime'] = pd.to_datetime(\n",
    "    students_schedule_filt['start_time'], \n",
    "    errors='coerce', \n",
    "    format='%H:%M:%S' \n",
    ").fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "students_schedule_filt.loc[:, 'end_datetime'] = pd.to_datetime(\n",
    "    students_schedule_filt['end_time'], \n",
    "    errors='coerce', \n",
    "    format='%H:%M:%S'  \n",
    ").fillna(pd.Timestamp('1900-01-01 23:59:00'))\n",
    "\n",
    "students_schedule_filt['duration'] = students_schedule_filt['end_datetime'] - students_schedule_filt['start_datetime']\n",
    "\n",
    "hour_activity_proportion = construct_observed_ToD_activity_frequency(students_schedule_filt, 'type')\n",
    "plot_observed_ToD_activity_frequency(hour_activity_proportion)\n",
    "# orig_sched_samp_filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_activity_proportions(orig_sched_samp_filt, 'type', p=True)\n",
    "calculate_mean_duration_hm(orig_sched_samp_filt, 'type', p=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_scenari_distributions(scenari):\n",
    "\n",
    "#     for index, scenario in enumerate(scenari):\n",
    "\n",
    "#         if index == 1:\n",
    "#             break\n",
    "\n",
    "#         df = json_to_flat_dataframe(f\"Data/Generated/{scenario}.json\")\n",
    "#         df_clean = process_schedules_json(df)\n",
    "\n",
    "#         df_clean.loc[:, 'start_datetime'] = pd.to_datetime(\n",
    "#             results_clean['start'], \n",
    "#             errors='coerce', \n",
    "#             format='%H:%M:%S' \n",
    "#         ).fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "#         df_clean.loc[:, 'end_datetime'] = pd.to_datetime(\n",
    "#             results_clean['end_time'], \n",
    "#             errors='coerce', \n",
    "#             format='%H:%M:%S'  \n",
    "#         ).fillna(pd.Timestamp('1900-01-01 23:59:00'))\n",
    "\n",
    "#         hour_activity_proportion =construct_observed_ToD_activity_frequency(df_clean, 'group')\n",
    "#         plot_observed_ToD_activity_frequency(hour_activity_proportion, scenario)\n",
    "\n",
    "# compare_scenari_distributions(SCENARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schedule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
